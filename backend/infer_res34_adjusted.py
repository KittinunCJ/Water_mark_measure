#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
infer_oneclick_piecewise_auto.py  (all-in-one, 2-decimal, auto piecewise)

- YOLOv8-seg ‚Üí ‡∏™‡∏Å‡∏±‡∏î waterline + ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á (px)
- Fusion model (+scaler.json ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) ‚Üí ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏π‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏°‡∏ï‡∏£
- ‡∏õ‡∏±‡∏î‡∏ú‡∏• 2 ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÄ‡∏™‡∏°‡∏≠
- Auto piecewise calibration:
    * ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ GT: ‡πÅ‡∏ö‡πà‡∏á‡∏ä‡πà‡∏ß‡∏á‡∏î‡πâ‡∏ß‡∏¢ error=(y_pred - y_true) ‡∏ó‡∏µ‡πà p33/p66 ‡πÅ‡∏•‡πâ‡∏ß fit y_true=a*y_pred+b ‡πÅ‡∏¢‡∏Å‡∏ä‡πà‡∏ß‡∏á
    * ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ GT: ‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö (‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å --auto_piecewise by_pred ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏ö‡πà‡∏á‡∏ä‡πà‡∏ß‡∏á‡∏ï‡∏≤‡∏° y_pred ‡πÅ‡∏ï‡πà a=1,b=0)
- ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Å‡∏±‡∏ö GT (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ): MAE/RMSE/R¬≤ (RAW vs ADJ)
- ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô CSV ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå adjust_bin, a_used, b_used

‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ (‡∏Ñ‡πà‡∏≤ default ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏´‡πâ‡πÅ‡∏•‡πâ‡∏ß):
python infer_oneclick_piecewise_auto.py
"""

import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"  # workaround OpenMP duplicate (Windows)

from pathlib import Path
import argparse, json
import numpy as np
import pandas as pd
from PIL import Image
import cv2
import torch
import torch.nn as nn
import torchvision.transforms as T
import torchvision.models as models
from ultralytics import YOLO

# ===== DEFAULT PATHS (‡πÅ‡∏Å‡πâ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì) =====
DEFAULT_WEIGHTS   = r"E:\499_cnx_flood_detect\YOLO_Flood_seg\runs\segment\train_PinFlood_segment\weights\best.pt"
DEFAULT_IMAGE_DIR = r"E:\499_cnx_flood_detect\YOLO_Flood_seg\Pin_Flood\remove_IQR_outlier_Pinflood_dataset\images\val"
DEFAULT_CKPT      = r"E:\499_cnx_flood_detect\YOLO_Flood_seg\Pin_Flood\modelres34_out\fusion_resnet34_best.pt"  # ckpt ‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡∏î‡πâ‡∏ß‡∏¢ res34 (‡∏°‡∏µ meta backbone ‡πÉ‡∏ô ckpt)
DEFAULT_SCALER    = r"E:\499_cnx_flood_detect\YOLO_Flood_seg\Pin_Flood\modelres34_out\scaler.json"               # ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡πá‡∏£‡∏±‡∏ô‡πÑ‡∏î‡πâ (‡∏Ç‡πâ‡∏≤‡∏° scaling)
DEFAULT_OUT       = r"E:\499_cnx_flood_detect\YOLO_Flood_seg\Pin_Flood\Prediction\result_modelmk1_res34\val2.2_predictions_Calib_scaled.csv"
DEFAULT_GT        = r"E:\499_cnx_flood_detect\YOLO_Flood_seg\Pin_Flood\All_Pin_Flood.csv"

WATER_CLASS = "Water_mark"
REF_CLASSES = ["Electric_pole","Brick","Motorcycle","Sedan_car","Pickup_car","SUV_car","Van_car","Pylon"]

# ---------- rounding helper ----------
def r2(x, n=2):
    return np.round(np.array(x, dtype=float), n)

# ---------- YOLO utils ----------
def combine_water_masks(result, target_class_name=WATER_CLASS):
    H, W = result.orig_img.shape[:2]
    names = result.names
    wm = np.zeros((H, W), np.uint8)
    if result.masks is None:
        return wm
    for i, cid in enumerate(result.boxes.cls.tolist()):
        if names[int(cid)] != target_class_name:
            continue
        m = result.masks.data[i].cpu().numpy()
        m = (m * 255).astype(np.uint8)
        m = cv2.resize(m, (W, H), interpolation=cv2.INTER_NEAREST)
        wm = cv2.bitwise_or(wm, m)
    return wm

def clean_mask(mask, min_area=200, k_close=5, k_open=3):
    if mask is None or mask.max() == 0:
        return mask
    kc = cv2.getStructuringElement(cv2.MORPH_RECT, (k_close, k_close))
    ko = cv2.getStructuringElement(cv2.MORPH_RECT, (k_open,  k_open))
    m = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kc)
    m = cv2.morphologyEx(m, cv2.MORPH_OPEN,  ko)
    num, labs, stats, _ = cv2.connectedComponentsWithStats((m>0).astype(np.uint8), 8)
    out = np.zeros_like(m)
    for i in range(1, num):
        if stats[i, cv2.CC_STAT_AREA] >= min_area:
            out[labs==i] = 255
    return out

def waterline_y_curve(mask_binary, smooth_px=15, min_points_ratio=0.02):
    H, W = mask_binary.shape
    y_top = np.full(W, np.nan, dtype=np.float32)
    for x in range(W):
        ys = np.where(mask_binary[:, x] > 0)[0]
        if len(ys) > 0:
            y_top[x] = ys.min()
    good = ~np.isnan(y_top)
    if good.sum() < max(5, int(min_points_ratio * W)):
        return None, None
    xs = np.arange(W, dtype=np.float32)
    y_interp = np.interp(xs, xs[good], y_top[good]).astype(np.float32)
    k = smooth_px if smooth_px % 2 == 1 else smooth_px + 1
    y_smooth = cv2.GaussianBlur(y_interp.reshape(1, -1), (k, 1), 0).ravel()
    y_curve = np.clip(np.round(y_smooth).astype(int), 0, H-1)
    y_median = int(np.median(y_curve))
    return y_curve, y_median

def ref_sizes_from_result(result):
    names = result.names
    H, W = result.orig_img.shape[:2]
    values = {c: [] for c in REF_CLASSES}
    if result.boxes is None:
        return {f"{k}_px": np.nan for k in REF_CLASSES}
    use_mask_bbox = result.masks is not None
    for i, cid in enumerate(result.boxes.cls.tolist()):
        cls = names[int(cid)]
        if cls not in REF_CLASSES:
            continue
        if use_mask_bbox and i < len(result.masks.data):
            mask = (result.masks.data[i].cpu().numpy()*255).astype(np.uint8)
            mask = cv2.resize(mask, (W, H), interpolation=cv2.INTER_NEAREST)
            ys, xs = np.where(mask>0)
            if len(xs)==0: continue
            x0, x1 = xs.min(), xs.max()
            y0, y1 = ys.min(), ys.max()
        else:
            x0,y0,x1,y1 = result.boxes.xyxy[i].cpu().numpy().astype(int)
        width  = max(1, int(x1 - x0))
        height = max(1, int(y1 - y0))
        values[cls].append(width if cls=="Electric_pole" else height)
    out = {f"{k}_px": (float(np.mean(v)) if v else np.nan) for k, v in values.items()}
    return out

# ---------- Fusion model ----------
class FusionRegressor(nn.Module):
    def __init__(self, n_tab_features, backbone="res34"):
        super().__init__()
        if backbone == "res50":
            try:   self.cnn = models.resnet50(weights=None)
            except: self.cnn = models.resnet50(pretrained=False)
        elif backbone == "res18":
            try:   self.cnn = models.resnet18(weights=None)
            except: self.cnn = models.resnet18(pretrained=False)
        else:
            try:   self.cnn = models.resnet34(weights=None)
            except: self.cnn = models.resnet34(pretrained=False)
        in_feat = self.cnn.fc.in_features
        self.cnn.fc = nn.Identity()
        self.tab = nn.Sequential(
            nn.Linear(n_tab_features, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
        )
        self.head = nn.Sequential(
            nn.Linear(in_feat + 64, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 1)
        )
    def forward(self, img, x_tab):
        f_img = self.cnn(img)
        f_tab = self.tab(x_tab)
        return self.head(torch.cat([f_img, f_tab], dim=1)).squeeze(1)

def load_checkpoint(ckpt_path, default_img_size=384):
    ckpt = torch.load(ckpt_path, map_location="cpu")
    feat_names = ckpt.get("feature_names", None)
    img_size   = ckpt.get("img_size", default_img_size)
    backbone   = ckpt.get("backbone", "res34")
    model = FusionRegressor(n_tab_features=len(feat_names) if feat_names else 0, backbone=backbone)
    model.load_state_dict(ckpt["model"])
    model.eval()
    return model, feat_names, img_size

# ---------- feature-engineering ----------
BASE_COLS = ["pixel_height","H","y_median","W"]
REF_COLS  = ["Electric_pole_px","Brick_px","Motorcycle_px","Sedan_car_px",
             "Pickup_car_px","SUV_car_px","Van_car_px","Pylon_px"]
PERSPECTIVE_COLS = ["water_area_norm","wm_conf_mean","pole_conf_mean"]

def add_engineered_features(df: pd.DataFrame) -> pd.DataFrame:
    out = df.copy()
    H  = out.get("H", pd.Series(np.nan, index=out.index)).astype(float)
    YM = out.get("y_median", pd.Series(np.nan, index=out.index)).astype(float)
    PX = out.get("pixel_height", pd.Series(np.nan, index=out.index)).astype(float)
    out["pixel_height_norm"] = PX / np.where(H>0, H, np.nan)
    out["y_median_norm"]     = YM / np.where(H>0, H, np.nan)
    pole = out.get("Electric_pole_px", pd.Series(np.nan, index=out.index)).astype(float)
    out["pole_ratio"] = np.where((PX>0)&(~np.isnan(PX)), pole/np.where(PX>0, PX, np.nan), np.nan)
    exist_refs = [c for c in REF_COLS if c in out.columns]
    out["n_refs"] = out[exist_refs].notna().sum(axis=1).astype(float) if exist_refs else 0.0
    return out

def apply_scaler_df(feats: pd.DataFrame, scaler: dict) -> pd.DataFrame:
    cols = scaler["feature_names"]
    X = feats.reindex(columns=cols)
    mean = np.array(scaler["mean"], dtype=np.float64)
    std  = np.array(scaler["std"],  dtype=np.float64)
    X_scaled = (X - mean) / np.where(std>1e-12, std, 1.0)
    return X_scaled

def make_feature_matrix(df, feature_names, scaler=None):
    df2 = add_engineered_features(df)
    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ scaler ‚Üí ‡πÉ‡∏ä‡πâ‡∏°‡∏±‡∏ô, ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡πá‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏î‡∏¥‡∏ö (‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà scale)
    if scaler is not None:
        cols = scaler["feature_names"]
        feats = df2.reindex(columns=cols).copy()
        miss  = feats.isna().astype(float).add_prefix("isna_")
        feats = apply_scaler_df(feats, scaler).fillna(0.0)
        Xdf = pd.concat([feats, miss], axis=1)
    else:
        base = BASE_COLS + REF_COLS + ["pixel_height_norm","y_median_norm","pole_ratio","n_refs"]
        cols = [c for c in base if c in df2.columns]
        feats = df2[cols].copy()
        miss  = feats.isna().astype(float).add_prefix("isna_")
        Xdf = pd.concat([feats.fillna(0.0), miss], axis=1)

    for c in feature_names:
        if c not in Xdf.columns:
            Xdf[c] = 0.0
    Xdf = Xdf[feature_names]
    return Xdf.values.astype(np.float32), list(Xdf.columns)

def load_image_tensor(path, img_size):
    tf = T.Compose([
        T.Resize((img_size, img_size)),
        T.ToTensor(),
        T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),
    ])
    im = Image.open(path).convert("RGB")
    return tf(im).unsqueeze(0)

def norm_key(s: str):
    stem = Path(str(s)).stem
    parts = stem.split("_")
    return "_".join(parts[:2]) if len(parts) >= 2 else stem

# ---------- YOLO feature builder ----------
def build_features_with_yolo(weights, image_dir, imgsz=960, conf=0.35, iou=0.5):
    model = YOLO(weights)
    images = []
    for ext in ("*.jpg","*.jpeg","*.png","*.webp","*.bmp","*.tif","*.tiff"):
        images += list(Path(image_dir).glob(ext))
    images = sorted(images)

    rows = []
    for p in images:
        r = model.predict(source=str(p), imgsz=imgsz, conf=conf, iou=iou, verbose=False)[0]
        H, W = r.orig_img.shape[:2]

        wm = combine_water_masks(r, WATER_CLASS)
        wm_clean = clean_mask(wm, min_area=200, k_close=5, k_open=3)
        y_curve, y_med = waterline_y_curve(wm_clean, smooth_px=15)
        found = y_curve is not None
        pixel_height = (H-1) - y_med if found else np.nan

        ref_dict = ref_sizes_from_result(r)

        row = {"image": p.name, "H": H, "W": W,
               "y_median": (int(y_med) if found else np.nan),
               "pixel_height": (int(pixel_height) if found else np.nan),
               "found": bool(found)}
        row.update(ref_dict)
        rows.append(row)
    return pd.DataFrame(rows)

# ---------- Piecewise helpers ----------
def fit_line(y_pred, y_true):
    """fit y_true ~ a*y_pred + b (least squares)"""
    y_pred = np.asarray(y_pred, dtype=float)
    y_true = np.asarray(y_true, dtype=float)
    if len(y_pred) < 2:
        return 1.0, 0.0
    A = np.vstack([y_pred, np.ones_like(y_pred)]).T
    a, b = np.linalg.lstsq(A, y_true, rcond=None)[0]
    return float(a), float(b)

def split_bins(values, mode="percentile", p_lo=33, p_hi=66):
    """‡∏Ñ‡∏∑‡∏ô‡∏Ç‡∏≠‡∏ö‡∏ä‡πà‡∏ß‡∏á (lo,hi) 3 ‡∏ä‡πà‡∏ß‡∏á‡∏ï‡∏≤‡∏° percentiles"""
    v = np.asarray(values, dtype=float)
    lo = np.nanpercentile(v, p_lo)
    hi = np.nanpercentile(v, p_hi)
    return [(-1e18, lo), (lo, hi), (hi, 1e18)]

def assign_bin(val, bins):
    for i, (lo, hi) in enumerate(bins):
        if lo <= val < hi:
            return i
    return len(bins) - 1

# ---------- main ----------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--weights", default=DEFAULT_WEIGHTS)
    ap.add_argument("--images",  default=DEFAULT_IMAGE_DIR)
    ap.add_argument("--ckpt",    default=DEFAULT_CKPT)
    ap.add_argument("--scaler",  default=DEFAULT_SCALER,
                    help="‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏à‡∏∞‡πÉ‡∏ä‡πâ normalize ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ï‡∏≠‡∏ô‡πÄ‡∏ó‡∏£‡∏ô; ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏à‡∏∞‡∏Ç‡πâ‡∏≤‡∏° scaling")
    ap.add_argument("--out",     default=DEFAULT_OUT)
    ap.add_argument("--imgsz",   type=int, default=960)
    ap.add_argument("--conf",    type=float, default=0.35)
    ap.add_argument("--iou",     type=float, default=0.5)
    ap.add_argument("--gt",      default=DEFAULT_GT)
    ap.add_argument("--auto_piecewise", default="auto",
                    choices=["auto","off","by_pred","by_error"],
                    help="auto: ‡πÉ‡∏ä‡πâ by_error ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ GT, ‡πÑ‡∏°‡πà‡∏á‡∏±‡πâ‡∏ô off")
    args = ap.parse_args()

    image_dir = Path(args.images)
    out_path  = Path(args.out); out_path.parent.mkdir(parents=True, exist_ok=True)

    # 1) YOLO ‚Üí features
    print("üîç Running YOLO & building features ...")
    df = build_features_with_yolo(args.weights, image_dir, imgsz=args.imgsz, conf=args.conf, iou=args.iou)
    mask_found = df["found"].astype(str).str.lower().isin(["1","true","yes"]) if "found" in df.columns else np.ones(len(df), bool)
    df_use = df[mask_found].copy()
    if df_use.empty:
        df.to_csv(out_path, index=False, encoding="utf-8-sig", na_rep="NaN")
        print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡∏°‡∏µ waterline ‚Üí ‡πÄ‡∏ã‡∏ü‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏î‡∏¥‡∏ö‡πÅ‡∏ó‡∏ô"); print(f"‚úÖ saved: {out_path}")
        return

    # 2) ‡πÇ‡∏´‡∏•‡∏î Fusion model + (optional) scaler
    print("üß† Loading fusion model ...")
    model, feat_names, img_size = load_checkpoint(args.ckpt, default_img_size=384)
    scaler = None
    if args.scaler and Path(args.scaler).exists():
        try:
            scaler = json.load(open(args.scaler, "r", encoding="utf-8"))
            print("üß™ Loaded scaler.json")
        except Exception as e:
            print(f"‚ö†Ô∏è ‡πÇ‡∏´‡∏•‡∏î scaler.json ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‚Üí ‡∏Ç‡πâ‡∏≤‡∏° scaling ({e})")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    X, used_feature_names = make_feature_matrix(df_use, feature_names=feat_names, scaler=scaler)

    # 3) Predict (2-decimal)
    print("üöÄ Predicting ...")
    preds = []
    for i, row in df_use.iterrows():
        img_name = row["image"]
        p = image_dir / img_name
        if not p.exists():
            stem = Path(img_name).stem
            for ext in [".jpg",".jpeg",".png",".webp",".bmp",".tif",".tiff"]:
                cand = image_dir / f"{stem}{ext}"
                if cand.exists(): p = cand; break
        im = load_image_tensor(p, img_size).to(device)
        x = torch.tensor(X[df_use.index.get_loc(i)]).unsqueeze(0).to(device)
        with torch.no_grad():
            y = model(im, x).cpu().item()
        preds.append(y)
    preds = r2(preds, 2)

    # 4) DataFrame ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
    out_df = df_use[["image","H","W","y_median","pixel_height","found"] + [c for c in df_use.columns if c.endswith("_px")]].copy()
    out_df["pred_height_m_raw"] = preds.astype(float)

    # 5) ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° GT (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
    gt_path = Path(args.gt) if args.gt else None
    merged = None
    if gt_path and gt_path.exists():
        try:
            df_gt = pd.read_csv(gt_path)
            key_col = next((c for c in ["image","record","filename","id","CM_code"] if c in df_gt.columns), None)
            if key_col is not None and "height_m_true" in df_gt.columns:
                tmp = out_df[["image","pred_height_m_raw"]].copy()
                tmp["key"] = tmp["image"].apply(norm_key)
                df_gt["key"] = df_gt[key_col].apply(norm_key)
                merged = tmp.merge(df_gt[["key","height_m_true"]], on="key", how="inner")
                if len(merged) == 0:
                    merged = None
        except Exception as e:
            print(f"‚ö†Ô∏è ‡πÇ‡∏´‡∏•‡∏î/‡πÄ‡∏°‡∏¥‡∏£‡πå‡∏à GT ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")
            merged = None

    # 6) Auto piecewise
    mode = args.auto_piecewise
    if mode == "auto":
        mode = "by_error" if merged is not None else "off"

    did_adjust = False
    bin_names, a_used, b_used = None, None, None

    if mode == "by_error" and merged is not None:
        # ‡∏õ‡∏±‡∏î GT 2-decimal ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á
        merged["height_m_true"] = r2(merged["height_m_true"], 2)
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì error ‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏ö‡πà‡∏á bins
        err = merged["pred_height_m_raw"].values - merged["height_m_true"].values
        bins = split_bins(err, p_lo=33, p_hi=66)  # 3 ‡∏ä‡πà‡∏ß‡∏á
        # fit a,b ‡∏ï‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏á
        a_list, b_list = [], []
        labels = []
        # map key -> (a,b) later
        merged["bin_id"] = [assign_bin(e, bins) for e in err]
        for k in range(3):
            mask = (merged["bin_id"].values == k)
            if mask.sum() >= 2:
                a,b = fit_line(merged.loc[mask,"pred_height_m_raw"], merged.loc[mask,"height_m_true"])
            else:
                a,b = 1.0, 0.0
            a_list.append(a); b_list.append(b)

        key2ab = {}
        for k in range(3):
            key2ab[k] = (a_list[k], b_list[k])

     
        key_to_bin = dict(zip(merged["key"].values, merged["bin_id"].values))
        out_df["key"] = out_df["image"].apply(norm_key)
        out_df["adjust_bin"] = out_df["key"].map(key_to_bin)
        a_used = out_df["adjust_bin"].map({i:a_list[i] for i in range(3)}).fillna(1.0).values
        b_used = out_df["adjust_bin"].map({i:b_list[i] for i in range(3)}).fillna(0.0).values

        out_df["pred_height_m"] = r2(a_used * out_df["pred_height_m_raw"].values + b_used, 2)
       
        out_df["adjust_bin"] = out_df["adjust_bin"].map({0:"Low",1:"Mid",2:"High"}).fillna("NA")
        bin_names = ["Low","Mid","High"]
        did_adjust = True
        print(f"üîß Piecewise(by_error) applied; bins via error p33/p66. Params per bin:")
        for i,name in enumerate(bin_names):
            print(f"   - {name}: a={a_list[i]:.3f}, b={b_list[i]:.3f}")

    elif mode == "by_pred":
        # ‡πÅ‡∏ö‡πà‡∏á bins ‡∏à‡∏≤‡∏Å y_pred ‡πÄ‡∏≠‡∏á (‡πÑ‡∏°‡πà‡∏°‡∏µ GT ‡πÉ‡∏´‡πâ fit ‚Üí identity)
        y = out_df["pred_height_m_raw"].values
        bins = split_bins(y, p_lo=33, p_hi=66)
        out_df["adjust_bin"] = [assign_bin(v, bins) for v in y]
        out_df["pred_height_m"] = r2(out_df["pred_height_m_raw"].values, 2)  # a=1,b=0
        out_df["adjust_bin"] = out_df["adjust_bin"].map({0:"Low","1":"Mid","2":"High"}).fillna("NA")
        did_adjust = False  # no actual change
        print("‚ÑπÔ∏è by_pred mode (no GT): bins ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏à‡∏≤‡∏Å y_pred ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤ (a=1,b=0)")

    else:
        out_df["pred_height_m"] = r2(out_df["pred_height_m_raw"], 2)
        print("‚ÑπÔ∏è piecewise: off")

    # 7) Save CSV (‡∏õ‡∏±‡∏î‡πÄ‡∏•‡∏Ç‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç)
    for c in ["pred_height_m_raw","pred_height_m"] + [c for c in out_df.columns if c.endswith("_px")]:
        if c in out_df.columns: out_df[c] = r2(out_df[c], 2)
    if a_used is not None:
        out_df["a_used"] = r2(a_used, 3)
        out_df["b_used"] = r2(b_used, 3)
    out_df.to_csv(out_path, index=False, encoding="utf-8-sig", na_rep="NaN")
    print(f"‚úÖ saved: {out_path}")

    # 8) Metrics (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ GT)
    if merged is not None:
        # ‡∏£‡∏ß‡∏° pred ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤ merged ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metric
        # ‡∏ó‡∏≥ key ‡πÉ‡∏´‡πâ unique ‡∏Å‡πà‡∏≠‡∏ô (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏ï‡πà‡∏≠ key)
        m_adj = (
            out_df[["key", "pred_height_m"]]
            .dropna()
            .groupby("key", as_index=True)["pred_height_m"]
            .mean()
        )
        m_raw = (
            out_df[["key", "pred_height_m_raw"]]
            .dropna()
            .groupby("key", as_index=True)["pred_height_m_raw"]
            .mean()
        )

        # ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÅ‡∏£‡∏Å/‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÅ‡∏ó‡∏ô‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢:
        # m_adj = (out_df.dropna(subset=["pred_height_m"])
        #                 .drop_duplicates("key", keep="last")
        #                 .set_index("key")["pred_height_m"])
        # m_raw = (out_df.dropna(subset=["pred_height_m_raw"])
        #                 .drop_duplicates("key", keep="last")
        #                 .set_index("key")["pred_height_m_raw"])

        merged["pred_height_m_adj"] = merged["key"].map(m_adj).astype(float)
        merged["pred_height_m_raw"] = merged["key"].map(m_raw).astype(float)

        def metric(y_true, y_pred):
            y_true = np.asarray(y_true, dtype=float)
            y_pred = np.asarray(y_pred, dtype=float)
            mae  = float(np.mean(np.abs(y_true - y_pred)))
            rmse = float(np.sqrt(np.mean((y_true - y_pred)**2)))
            ss_res = np.sum((y_true - y_pred)**2)
            ss_tot = np.sum((y_true - y_true.mean())**2)
            r2s = float(1 - ss_res/ss_tot) if ss_tot > 0 else np.nan
            return mae, rmse, r2s

        mae_r, rmse_r, r2_r = metric(merged["height_m_true"], merged["pred_height_m_raw"])
        mae_c, rmse_c, r2_c = metric(merged["height_m_true"], merged["pred_height_m_adj"])

        print(f"\nüìè Metrics on GT (matched {len(merged)} imgs) [2-decimal]:")
        print(f"  RAW : MAE={mae_r:.2f} m  RMSE={rmse_r:.2f} m  R^2={r2_r:.2f}")
        print(f"  ADJ : MAE={mae_c:.2f} m  RMSE={rmse_c:.2f} m  R^2={r2_c:.2f}")

        merged.to_csv(out_path.with_name(out_path.stem + "_with_gt.csv"),
                      index=False, encoding="utf-8-sig")
        print(f"üìù saved: {out_path.with_name(out_path.stem + '_with_gt.csv')}")
    else:
        print("‚ÑπÔ∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ/‡πÄ‡∏°‡∏¥‡∏£‡πå‡∏à GT ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: ‡πÅ‡∏™‡∏î‡∏á‡πÑ‡∏î‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ú‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢")


if __name__ == "__main__":
    main()
